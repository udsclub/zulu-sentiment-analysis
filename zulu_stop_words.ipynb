{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def import_data(path, sep=\"|\"):\n",
    "    return pd.read_csv(path, sep)\n",
    "\n",
    "def save_trained_model(model, path):\n",
    "    joblib.dump(model, path)\n",
    "\n",
    "\n",
    "def __drop_duplicates(df):\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "\n",
    "def __balance_data(df):\n",
    "    df_positive = df[df.label == 1]\n",
    "    df_negative = df[df.label == 0]\n",
    "    pos_vs_neg = len(df_positive) - len(df_negative)\n",
    "    if pos_vs_neg > 0:\n",
    "        drop_indices = np.random.choice(df_positive.index, pos_vs_neg, replace=False)\n",
    "        df_positive = df_positive.drop (drop_indices)\n",
    "    elif pos_vs_neg < 0:\n",
    "        pos_vs_neg *= -1\n",
    "        drop_indices = np.random.choice(df_negative.index, pos_vs_neg, replace=False)\n",
    "        df_negative = df_negative.drop (drop_indices)\n",
    "    else:\n",
    "         return df   \n",
    "    return pd.concat([df_positive, df_negative])\n",
    "\n",
    "\n",
    "def __perform_stemming(review):\n",
    "    stemmer = PorterStemmer()\n",
    "    return \"\".join([stemmer.stem(word) for word in review.split()])\n",
    "\n",
    "\n",
    "def __drop_non_english(df):\n",
    "    drop_indices= []\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            if detect(row['text']) != \"en\":\n",
    "                drop_indices.append (index)\n",
    "        except Exception:\n",
    "             drop_indices.append (index)\n",
    "    return df.drop(drop_indices)\n",
    "\n",
    "    \n",
    "def __is_numerical(token):\n",
    "    return str.isnumeric(token)\n",
    "\n",
    "\n",
    "def preprocessing(df, balance_data=False, drop_non_english=False, drop_duplicates=False, stemming=False, \n",
    "                  replace_numerical=False, replacement_of_numerical=\"NUMERICAL_TOKEN\", lowercase=False):\n",
    "    if balance_data:\n",
    "        df = __balance_data(df)\n",
    "    if drop_duplicates:\n",
    "        df = __drop_duplicates(df)\n",
    "    if drop_non_english:\n",
    "        df = __drop_non_english(df)\n",
    "    def tbt_cleaning(review):\n",
    "        cleaned_tokens = []\n",
    "        stemmer = None\n",
    "        if stemming:\n",
    "            stemmer = PorterStemmer()\n",
    "        tokens = nltk.word_tokenize(review)\n",
    "        for token in tokens:\n",
    "            if stemming:\n",
    "                token = stemmer.stem(token)\n",
    "            if replace_numerical and __is_numerical(token):\n",
    "                token = replacement_of_numerical\n",
    "            if lowercase:\n",
    "                token = token.lower()\n",
    "            cleaned_tokens.append(token)\n",
    "        return ' '.join(cleaned_tokens)\n",
    "    if stemming or replace_numerical or lowercase:\n",
    "        return df.text.apply(tbt_cleaning)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152610"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dr = import_data('reviews_rt_all.csv')\n",
    "di = import_data('imdb_small.csv')\n",
    "frames = [dr, di]\n",
    "df = pd.concat(frames)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['last_22'] = df.text.str.split().apply(lambda x:  ' '.join(x for x in x[-22:]))\n",
    "df['text'] = df['last_22']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = preprocessing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = df.label\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.text, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STOPWORDS = ['the','of', 'is', 'im', 'that', 'it', 'this', 'for', 'with',  'film','you','movie','on', 'was', 'an', 'have',\n",
    "           'are', 'one', 'at', 'its', 'his', 'from', 'all', 'like', 'more']\n",
    "#the, of, a, and, to, is, in, that, as, it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now lets try putting this into a Count Vectorizer with default params.\n",
    "vectorizer = CountVectorizer(binary=True, ngram_range=(1,3), stop_words=STOPWORDS)\n",
    "X_train = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassiveAggressiveClassifier(C=0.001, class_weight=None, fit_intercept=False,\n",
       "              loss='hinge', n_iter=91, n_jobs=-1, random_state=None,\n",
       "              shuffle=False, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model\n",
    "classifier = PassiveAggressiveClassifier(C=0.001, fit_intercept = False, shuffle = False, n_iter = 91, n_jobs = -1)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create prediction label\n",
    "X_test = vectorizer.transform(X_test)\n",
    "predictions = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.798145599895\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy and score\n",
    "print (\"Accuracy:\", metrics.accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save model and vectorizer\n",
    "save_trained_model(classifier, 'model_PAC_RT_IMDB.pkl')\n",
    "save_trained_model(vectorizer, 'vectorizer_PAC_RT_IMDB.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate a non-NDFrame object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-281-dbe0991f13e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mMODEL_PATH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"model_PAC1.pkl\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mVECTORIZER_PATH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"vectorizer_PAC1.pkl\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mVECTORIZER_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMODEL_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\sromanenko\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\tools\\merge.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[1;32m   1449\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1451\u001b[0;31m                        copy=copy)\n\u001b[0m\u001b[1;32m   1452\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\sromanenko\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\pandas\\tools\\merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy)\u001b[0m\n\u001b[1;32m   1506\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mobjs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNDFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1508\u001b[0;31m                 \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cannot concatenate a non-NDFrame object\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1509\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m             \u001b[1;31m# consolidate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot concatenate a non-NDFrame object"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"model_PAC1.pkl\"\n",
    "VECTORIZER_PATH = \"vectorizer_PAC1.pkl\"\n",
    "df = pd.concat([X_test, y_test ], axis=1 )\n",
    "X = preprocessing(X_test, VECTORIZER_PATH)\n",
    "model = load_model(MODEL_PATH)\n",
    "y_pred = model.predict(X)\n",
    "display_accuracy(df.label, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from collections import Counter\n",
    "rows = Counter(chain.from_iterable(df.text.map(lambda x: str(x).split(\" \")))).most_common(40)\n",
    "STOPWORDS =[x[0] for x in rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.815411834087 ['the'] True\n",
      "Accuracy: 0.813970250967 ['a'] True\n",
      "Accuracy: 0.812495904593 ['and'] False\n",
      "Accuracy: 0.814822095538 ['of'] True\n",
      "Accuracy: 0.81269248411 ['to'] False\n",
      "Accuracy: 0.814265120241 ['is'] True\n",
      "Accuracy: 0.814003014219 ['in'] True\n",
      "Accuracy: 0.814953148549 ['that'] True\n",
      "Accuracy: 0.815116964812 ['it'] True\n",
      "Accuracy: 0.81449446301 ['this'] True\n",
      "Accuracy: 0.81413406723 ['for'] True\n",
      "Accuracy: 0.813937487714 ['as'] True\n",
      "Accuracy: 0.813970250967 ['I'] True\n",
      "Accuracy: 0.813806434703 ['but'] False\n",
      "Accuracy: 0.81521525457 ['with'] True\n",
      "Accuracy: 0.813970250967 ['The'] True\n",
      "Accuracy: 0.813970250967 ['/><br'] True\n",
      "Accuracy: 0.815018675054 ['film'] True\n",
      "Accuracy: 0.815346307581 ['you'] True\n",
      "Accuracy: 0.815248017823 ['movie'] True\n",
      "Accuracy: 0.814428936505 ['on'] True\n",
      "Accuracy: 0.813937487714 ['be'] True\n",
      "Accuracy: 0.811840639539 ['not'] False\n",
      "Accuracy: 0.814625516021 ['was'] True\n",
      "Accuracy: 0.814428936505 ['an'] True\n",
      "Accuracy: 0.814199593736 ['have'] True\n",
      "Accuracy: 0.814101303977 ['are'] True\n",
      "Accuracy: 0.814691042527 ['one'] True\n",
      "Accuracy: 0.814396173252 ['at'] True\n",
      "Accuracy: 0.814756569032 ['its'] True\n",
      "Accuracy: 0.813904724461 ['by'] True\n",
      "Accuracy: 0.813970250967 ['A'] True\n",
      "Accuracy: 0.81449446301 ['his'] True\n",
      "Accuracy: 0.814691042527 ['from'] True\n",
      "Accuracy: 0.814232356988 ['all'] True\n",
      "Accuracy: 0.814592752768 ['like'] True\n",
      "Accuracy: 0.814658279274 ['more'] True\n",
      "Accuracy: 0.813970250967 [\"it's\"] True\n",
      "Accuracy: 0.813904724461 ['about'] True\n",
      "Accuracy: 0.813904724461 ['or'] True\n"
     ]
    }
   ],
   "source": [
    "for word in STOPWORDS:\n",
    "    words = []\n",
    "    words.append(word)\n",
    "    df = preprocessing(df)\n",
    "    y = df.label\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.text, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    vectorizer = CountVectorizer(binary=True, ngram_range=(1,3), stop_words=words)\n",
    "    X_train = vectorizer.fit_transform(X_train)\n",
    "    classifier = PassiveAggressiveClassifier(C=0.001, fit_intercept = False, shuffle = False, n_iter = 91, n_jobs = -1)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    X_test = vectorizer.transform(X_test)\n",
    "    predictions = classifier.predict(X_test)\n",
    "    Accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "    check=False\n",
    "    if Accuracy>=0.813904724461:\n",
    "        check=True\n",
    "    print (\"Accuracy:\", Accuracy, words, check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-68-f025135c4141>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-68-f025135c4141>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Accuracy: 0.813904724461 -- без всего\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Accuracy: 0.813904724461 -- без всего"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Accuracy: 0.80024244807"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
