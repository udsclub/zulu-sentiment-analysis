{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import gensim\n",
    "from langdetect import detect\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def import_data(path, sep=\"|\"):\n",
    "    return pd.read_csv(path, sep)\n",
    "\n",
    "def save_trained_model(model, path):\n",
    "    joblib.dump(model, path)\n",
    "\n",
    "\n",
    "def __drop_duplicates(df):\n",
    "    return df.drop_duplicates()\n",
    "\n",
    "\n",
    "def __balance_data(df):\n",
    "    df_positive = df[df.label == 1]\n",
    "    df_negative = df[df.label == 0]\n",
    "    pos_vs_neg = len(df_positive) - len(df_negative)\n",
    "    if pos_vs_neg > 0:\n",
    "        drop_indices = np.random.choice(df_positive.index, pos_vs_neg, replace=False)\n",
    "        df_positive = df_positive.drop (drop_indices)\n",
    "    elif pos_vs_neg < 0:\n",
    "        pos_vs_neg *= -1\n",
    "        drop_indices = np.random.choice(df_negative.index, pos_vs_neg, replace=False)\n",
    "        df_negative = df_negative.drop (drop_indices)\n",
    "    else:\n",
    "         return df   \n",
    "    return pd.concat([df_positive, df_negative])\n",
    "\n",
    "\n",
    "def __perform_stemming(review):\n",
    "    stemmer = PorterStemmer()\n",
    "    return \"\".join([stemmer.stem(word) for word in review.split()])\n",
    "\n",
    "\n",
    "def __drop_non_english(df):\n",
    "    drop_indices= []\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            if detect(row['text']) != \"en\":\n",
    "                drop_indices.append (index)\n",
    "        except Exception:\n",
    "             drop_indices.append (index)\n",
    "    return df.drop(drop_indices)\n",
    "\n",
    "    \n",
    "def __is_numerical(token):\n",
    "    return str.isnumeric(token)\n",
    "\n",
    "\n",
    "def preprocessing(df, balance_data=False, drop_non_english=False, drop_duplicates=False, stemming=False, \n",
    "                  replace_numerical=False, replacement_of_numerical=\"NUMERICAL_TOKEN\", lowercase=False):\n",
    "    if balance_data:\n",
    "        df = __balance_data(df)\n",
    "    if drop_duplicates:\n",
    "        df = __drop_duplicates(df)\n",
    "    if drop_non_english:\n",
    "        df = __drop_non_english(df)\n",
    "    def tbt_cleaning(review):\n",
    "        cleaned_tokens = []\n",
    "        stemmer = None\n",
    "        if stemming:\n",
    "            stemmer = PorterStemmer()\n",
    "        tokens = nltk.word_tokenize(review)\n",
    "        for token in tokens:\n",
    "            if stemming:\n",
    "                token = stemmer.stem(token)\n",
    "            if replace_numerical and __is_numerical(token):\n",
    "                token = replacement_of_numerical\n",
    "            if lowercase:\n",
    "                token = token.lower()\n",
    "            cleaned_tokens.append(token)\n",
    "        return ' '.join(cleaned_tokens)\n",
    "    if stemming or replace_numerical or lowercase:\n",
    "        df.text = df.text.apply(tbt_cleaning)\n",
    "        return df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102610"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = import_data('reviews_rt_all.csv')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Takes last n words if naive is set True. Otherwise, takes firs n/2 words and last n/2 words.\n",
    "def take_n_words(review, naive=False, n=20):\n",
    "    tokens = nltk.word_tokenize(review) \n",
    "    l = len(tokens)\n",
    "    if l <= n:\n",
    "        return tokens\n",
    "    else:\n",
    "        if naive:\n",
    "            tokens.revers()\n",
    "            return tokens[:n]\n",
    "        else:\n",
    "            first = []\n",
    "            last = []\n",
    "            total = 0\n",
    "            for i in range(0, int(n/2)):\n",
    "                if total < n:\n",
    "                    first.append(tokens[i])\n",
    "                    total += 1\n",
    "                if total < n:\n",
    "                    last.append(tokens[l - 1 - i])\n",
    "                    total += 1\n",
    "            last.reverse()\n",
    "            return first + last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['n_words'] = df['text'].map(lambda t: take_n_words(t, naive=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vectorize(review_tokens, n=20):\n",
    "    feature_vector = np.array([])\n",
    "    for token in review_tokens:\n",
    "        vector = None\n",
    "        try:\n",
    "            vector = word2vec[token]\n",
    "        except:\n",
    "            vector = np.zeros(300)\n",
    "        feature_vector = np.concatenate([feature_vector, vector])\n",
    "    tokens_number = len(review_tokens)\n",
    "    if tokens_number < n:\n",
    "        feature_vector = np.concatenate([feature_vector, np.zeros(300 * (n - tokens_number))])\n",
    "    return feature_vector.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['features'] = df['n_words'].map(lambda w: vectorize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create validation and training dataset\n",
    "y = df.label\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.stack(df.features), y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassiveAggressiveClassifier(C=0.001, class_weight=None, fit_intercept=False,\n",
       "              loss='hinge', n_iter=91, n_jobs=-1, random_state=None,\n",
       "              shuffle=False, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model\n",
    "classifier = PassiveAggressiveClassifier(C=0.001, fit_intercept = False, shuffle = False, n_iter = 91, n_jobs = -1)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create prediction label\n",
    "predictions = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.734869895722\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy and score\n",
    "print (\"Accuracy:\", metrics.accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "save_trained_model(classifier, 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [testenv]",
   "language": "python",
   "name": "Python [testenv]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
